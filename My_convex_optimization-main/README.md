# My_convex_optimization 
Project of qwasar
[//]: # (# Welcome to My Convex Optimization)

[//]: # (One of the main challenges of Data Science and more specifically in Machine Learning is the performance measure.)

[//]: # (How to measure performance efficiently so that our model predictions meet the business objectives?)

[//]: # (This project is meant to be an introduction to some convex optimization tools and to implement your own optimization algorithm.)

[//]: # ()
[//]: # (Gradient Descent Methods)

[//]: # (Simplex algorithm)

[//]: # ()
[//]: # (## Task)

[//]: # (→ Plot this function to get a feel of what it looks like !)

[//]: # (→ Write a simple dichotomous algorithm &#40;bisection method&#41; to find the zero of a function.)

[//]: # (→ Use find_root to find the root of f prime.)

[//]: # (→ How does the learning rate influence the efficiency of the algorithm? What happens if it is very small? What if it is very big?)

[//]: # (→ Write a simple gradient descent function which finds the minimum of a function f)

[//]: # (→ Initialize A, b, and c as numpy arrays)

[//]: # (→ Solve the linear problem using simplex method)

[//]: # (→ Is the solution you found located on the edge of the polytope? Why?)

[//]: # ()
[//]: # (## Description)

[//]: # (1. I learned gradient descent algorithms and Brent methods.)

[//]: # (2. Then I got information about the newton raphson method.)

[//]: # (3. I had a lot of confusion to solve the problem.)

[//]: # (Gradient Descent)

[//]: # (In mathematics, gradient descent &#40;also often called steepest descent&#41; is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function.)

[//]: # (The idea is to take repeated steps in the opposite direction of the gradient &#40;or approximate gradient&#41; of the function at the current point, because this is the direction of steepest descent.)

[//]: # (Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.)

[//]: # (Brent)

[//]: # (In numerical analysis, Brent's method is a hybrid root-finding algorithm combining the bisection method, the secant method and inverse quadratic interpolation.)

[//]: # (It has the reliability of bisection but it can be as quick as some of the less-reliable methods. )

[//]: # (The algorithm tries to use the potentially fast-converging secant method or inverse quadratic interpolation if possible, but it falls back to the more robust bisection method if necessary. )

[//]: # (Brent's method is due to Richard Brent and builds on an earlier algorithm by Theodorus Dekker.)

[//]: # (Consequently, the method is also known as the Brent–Dekker method.)

[//]: # (## Installation)

[//]: # ($ pip install -r requirements.txt)

[//]: # (## Usage)

[//]: # ($ python my_convex_optimization.py)

[//]: # ()
[//]: # (### The Core Team)

[//]: # ()
[//]: # ()
[//]: # (<span><i>Made at <a href='https://qwasar.io'>Qwasar Silicon Valley</a></i></span>)

[//]: # (<span><img alt='Qwasar Silicon Valley Logo' src='https://storage.googleapis.com/qwasar-public/qwasar-logo_50x50.png' width='20px'></span>)

[//]: # ()
